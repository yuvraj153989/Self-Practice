{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b798feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy is a NLP library similar to NLTK, but the difference is spacy is OOP tool, \n",
    "# while nltk is proper string processing library\n",
    "# spacy is most efficient, nltk provides customization and many algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd2946c",
   "metadata": {},
   "outputs": [],
   "source": [
    " import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d995f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii.\n",
      "I'm Dr. Yuvraj Singh.\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"Hii. I'm Dr. Yuvraj Singh.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "\n",
    "#printing sentences, sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172e1d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii\n",
      ".\n",
      "I\n",
      "'m\n",
      "Dr.\n",
      "Yuvraj\n",
      "Singh\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "        \n",
    "#printing word from sentence, word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a71a0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c015a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hii.', \"I'm Dr. Yuvraj Singh.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"Hii. I'm Dr. Yuvraj Singh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3bd1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07305f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2b3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d7dda52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[]\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        urls.append(token.text)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d6cc013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two $', '500 €']\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc=nlp(transactions)\n",
    "extracted_transactions = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = token.nbor(1)\n",
    "        if next_token and next_token.is_currency:\n",
    "            extracted_transactions.append(f\"{token} {next_token}\")\n",
    "\n",
    "print(extracted_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347a1a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captin\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.blank('en')\n",
    "doc=nlp(\"Captin america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9614918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names  #pipelines are pre defined, or we need to create those\n",
    "# for example 'en','hi','en_core_web_sm' etc are pre defined pipelines.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c6cfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use en_core_web_sm this one\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.pipe_names\n",
    "\n",
    "#it has this this components.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dbdacb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captin  |  PROPN  |  Captin\n",
      "america  |  PROPN  |  america\n",
      "ate  |  VERB  |  eat\n",
      "100  |  NUM  |  100\n",
      "$  |  NUM  |  $\n",
      "of  |  ADP  |  of\n",
      "samosa  |  PROPN  |  samosa\n",
      ".  |  PUNCT  |  .\n",
      "Then  |  ADV  |  then\n",
      "he  |  PRON  |  he\n",
      "said  |  VERB  |  say\n",
      "I  |  PRON  |  I\n",
      "can  |  AUX  |  can\n",
      "do  |  VERB  |  do\n",
      "this  |  PRON  |  this\n",
      "all  |  DET  |  all\n",
      "day  |  NOUN  |  day\n",
      ".  |  PUNCT  |  .\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Captin america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token,\" | \",token.pos_,\" | \",token.lemma_)\n",
    "    \n",
    "#lemma gives u the base word, for ex: ate is a past tense, base one is eat, had -> have.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "581d6a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla -> ORG -> Companies, agencies, institutions, etc.\n",
      "Acquire Twitter -> PERSON -> People, including fictional\n",
      "90 billions -> MONEY -> Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "#use of component ner, it basically recognizing the entities..\n",
    "doc=nlp(\"Tesla is going to Acquire Twitter for whopping amount of 90 billions\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"->\",ent.label_,'->',spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ae45c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Acquire Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for whopping amount of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    90 billions\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent')\n",
    "\n",
    "#can even highlight the entities.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ce7bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommy Hilfiger -> PERSON -> People, including fictional\n",
      "Tommy Hilfiger -> PERSON -> People, including fictional\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tommy Hilfiger\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded his Company \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tommy Hilfiger\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc=nlp(\"Tommy Hilfiger founded his Company Tommy Hilfiger\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"->\",ent.label_,'->',spacy.explain(ent.label_))\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06dd326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming and lemmatization in Spacy,\n",
    "# here, stemming is not supported by spacy because lemmatization is more sophisticated and complex, so they wrapped up \n",
    "# all in lemmatization, while nltk has both components..\n",
    "# in NLP, accuracy(lemmatization) >> acc(stemming), so spacy simply neglected less accurate component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb86f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d261839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating -> eat\n",
      "reading -> read\n",
      "curable -> curabl\n",
      "ate -> ate\n",
      "meeting -> meet\n",
      "ability -> abil\n",
      "durable -> durabl\n",
      "saw -> saw\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "words=['eating','reading','curable','ate','meeting','ability','durable','saw']\n",
    "for word in words:\n",
    "    print(word,\"->\",stemmer.stem(word))\n",
    "    \n",
    "#see, it is so less accurate, can't even recognize for 'ate', just using fixed rules..\n",
    "#still some ppl use this coz of speed, no pre req. knowledge requirement, works on bunch of rules, can add value to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d228da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating -> eat\n",
      "eats -> eat\n",
      "reading -> read\n",
      "curable -> curable\n",
      "ate -> ate\n",
      "meeting -> meeting\n",
      "ability -> ability\n",
      "durable -> durable\n",
      "saw -> saw\n",
      "came -> come\n",
      "had -> have\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(\"eating eats reading curable ate meeting ability durable saw came had\")\n",
    "for token in doc:\n",
    "    print(token,\"->\",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f42ec499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d26524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro -> Brother\n",
      "Bruh -> Brother\n",
      "Duhh -> Duhh\n",
      "Brother -> Brother\n",
      "Brahh -> Brother\n"
     ]
    }
   ],
   "source": [
    "ar=nlp.get_pipe(\"attribute_ruler\")\n",
    "ar.add([[{'TEXT':\"Bro\"}],[{'TEXT':\"Bruh\"}],[{'TEXT':\"Brahh\"}]],{\"LEMMA\":\"Brother\"}) #adding rule..\n",
    "doc=nlp(\"Bro Bruh Duhh Brother Brahh\")\n",
    "for token in doc:\n",
    "    print(token,\"->\",token.lemma_)\n",
    "    \n",
    "#here bro, brahh, bruhh all are same, but this ,odel wont get it, still we can customize it as per our need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3c8065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "painting -> paint\n",
      "walking -> walk\n",
      "dressing -> dress\n",
      "likely -> like\n",
      "children -> children\n",
      "whom -> whom\n",
      "good -> good\n",
      "ate -> ate\n",
      "fishing -> fish\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
    "for word in lst_words:\n",
    "    print(word,'->',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc8f1a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "painting -> paint\n",
      "walking -> walk\n",
      "dressing -> dress\n",
      "likely -> likely\n",
      "children -> child\n",
      "who -> who\n",
      "good -> good\n",
      "ate -> eat\n",
      "fishing -> fishing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
    "for token in doc:\n",
    "    print(token,'->',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59b07193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "painting\n",
      "walking\n",
      "dressing\n",
      "likely\n",
      "children\n",
      "whom\n",
      "good\n",
      "ate\n",
      "fishing\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\"\n",
    "for word in lst_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de5bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon -> PROPN -> proper noun\n",
      "flew -> VERB -> verb\n",
      "to -> ADP -> adposition\n",
      "mars -> NOUN -> noun\n",
      "yesterday -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "he -> PRON -> pronoun\n",
      "was -> AUX -> auxiliary\n",
      "so -> ADV -> adverb\n",
      "quick -> ADJ -> adjective\n",
      "and -> CCONJ -> coordinating conjunction\n",
      "also -> ADV -> adverb\n",
      "carried -> VERB -> verb\n",
      "vadapav -> NOUN -> noun\n",
      "with -> ADP -> adposition\n",
      "him -> PRON -> pronoun\n"
     ]
    }
   ],
   "source": [
    "#Part of Speech in Spacy .pos_\n",
    "\n",
    "doc=nlp(\"Elon flew to mars yesterday, he was so quick and also carried vadapav with him\")\n",
    "for token in doc:\n",
    "    print(token,'->',token.pos_,'->',spacy.explain(token.pos_))\n",
    "    \n",
    "#with proper Exlanation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ea9933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp('''SEATTLE--(BUSINESS WIRE)-- Amazon.com, Inc. (NASDAQ: AMZN) today announced financial results for its second quarter ended June 30, 2023.\n",
    "\n",
    "Net sales increased 11% to $134.4 billion in the second quarter, compared with $121.2 billion in second quarter 2022. Excluding the $0.3 billion unfavorable impact from year-over-year changes in foreign exchange rates throughout the quarter, net sales increased 11% compared with second quarter 2022.\n",
    "North America segment sales increased 11% year-over-year to $82.5 billion.\n",
    "International segment sales increased 10% year-over-year to $29.7 billion.\n",
    "AWS segment sales increased 12% year-over-year to $22.1 billion.\n",
    "Operating income increased to $7.7 billion in the second quarter, compared with $3.3 billion in second quarter 2022.\n",
    "North America segment operating income was $3.2 billion, compared with an operating loss of $0.6 billion in second quarter 2022.\n",
    "International segment operating loss was $0.9 billion, compared with an operating loss of $1.8 billion in second quarter 2022.\n",
    "AWS segment operating income was $5.4 billion, compared with operating income of $5.7 billion in second quarter 2022.\n",
    "Net income was $6.7 billion in the second quarter, or $0.65 per diluted share, compared with a net loss of $2.0 billion, or $0.20 per diluted share, in second quarter 2022.\n",
    "Second quarter 2023 net income includes a pre-tax valuation gain of $0.2 billion included in non-operating expense from the common stock investment in Rivian Automotive, Inc., compared to a pre-tax valuation loss of $3.9 billion from the investment in second quarter 2022.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96e81ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEATTLE--(BUSINESS -> PROPN -> proper noun\n",
      "WIRE)-- -> PROPN -> proper noun\n",
      "Amazon.com -> PROPN -> proper noun\n",
      ", -> PUNCT -> punctuation\n",
      "Inc. -> PROPN -> proper noun\n",
      "( -> PUNCT -> punctuation\n",
      "NASDAQ -> PROPN -> proper noun\n",
      ": -> PUNCT -> punctuation\n",
      "AMZN -> PROPN -> proper noun\n",
      ") -> PUNCT -> punctuation\n",
      "today -> NOUN -> noun\n",
      "announced -> VERB -> verb\n",
      "financial -> ADJ -> adjective\n",
      "results -> NOUN -> noun\n",
      "for -> ADP -> adposition\n",
      "its -> PRON -> pronoun\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "ended -> VERB -> verb\n",
      "June -> PROPN -> proper noun\n",
      "30 -> NUM -> numeral\n",
      ", -> PUNCT -> punctuation\n",
      "2023 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      "\n",
      " -> SPACE -> space\n",
      "Net -> ADJ -> adjective\n",
      "sales -> NOUN -> noun\n",
      "increased -> VERB -> verb\n",
      "11 -> NUM -> numeral\n",
      "% -> NOUN -> noun\n",
      "to -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "134.4 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "the -> DET -> determiner\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "121.2 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "Excluding -> VERB -> verb\n",
      "the -> DET -> determiner\n",
      "$ -> SYM -> symbol\n",
      "0.3 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "unfavorable -> ADJ -> adjective\n",
      "impact -> NOUN -> noun\n",
      "from -> ADP -> adposition\n",
      "year -> NOUN -> noun\n",
      "- -> PUNCT -> punctuation\n",
      "over -> ADP -> adposition\n",
      "- -> PUNCT -> punctuation\n",
      "year -> NOUN -> noun\n",
      "changes -> NOUN -> noun\n",
      "in -> ADP -> adposition\n",
      "foreign -> ADJ -> adjective\n",
      "exchange -> NOUN -> noun\n",
      "rates -> NOUN -> noun\n",
      "throughout -> ADP -> adposition\n",
      "the -> DET -> determiner\n",
      "quarter -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "net -> ADJ -> adjective\n",
      "sales -> NOUN -> noun\n",
      "increased -> VERB -> verb\n",
      "11 -> NUM -> numeral\n",
      "% -> NOUN -> noun\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "North -> PROPN -> proper noun\n",
      "America -> PROPN -> proper noun\n",
      "segment -> NOUN -> noun\n",
      "sales -> NOUN -> noun\n",
      "increased -> VERB -> verb\n",
      "11 -> NUM -> numeral\n",
      "% -> NOUN -> noun\n",
      "year -> NOUN -> noun\n",
      "- -> PUNCT -> punctuation\n",
      "over -> ADP -> adposition\n",
      "- -> PUNCT -> punctuation\n",
      "year -> NOUN -> noun\n",
      "to -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "82.5 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "International -> ADJ -> adjective\n",
      "segment -> NOUN -> noun\n",
      "sales -> NOUN -> noun\n",
      "increased -> VERB -> verb\n",
      "10 -> NUM -> numeral\n",
      "% -> NOUN -> noun\n",
      "year -> NOUN -> noun\n",
      "- -> PUNCT -> punctuation\n",
      "over -> ADP -> adposition\n",
      "- -> PUNCT -> punctuation\n",
      "year -> NOUN -> noun\n",
      "to -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "29.7 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "AWS -> PROPN -> proper noun\n",
      "segment -> NOUN -> noun\n",
      "sales -> NOUN -> noun\n",
      "increased -> VERB -> verb\n",
      "12 -> NUM -> numeral\n",
      "% -> NOUN -> noun\n",
      "year -> NOUN -> noun\n",
      "- -> PUNCT -> punctuation\n",
      "over -> ADP -> adposition\n",
      "- -> PUNCT -> punctuation\n",
      "year -> NOUN -> noun\n",
      "to -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "22.1 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "Operating -> VERB -> verb\n",
      "income -> NOUN -> noun\n",
      "increased -> VERB -> verb\n",
      "to -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "7.7 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "the -> DET -> determiner\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "3.3 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "North -> PROPN -> proper noun\n",
      "America -> PROPN -> proper noun\n",
      "segment -> NOUN -> noun\n",
      "operating -> NOUN -> noun\n",
      "income -> NOUN -> noun\n",
      "was -> AUX -> auxiliary\n",
      "$ -> SYM -> symbol\n",
      "3.2 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "an -> DET -> determiner\n",
      "operating -> NOUN -> noun\n",
      "loss -> NOUN -> noun\n",
      "of -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "0.6 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "International -> ADJ -> adjective\n",
      "segment -> NOUN -> noun\n",
      "operating -> NOUN -> noun\n",
      "loss -> NOUN -> noun\n",
      "was -> AUX -> auxiliary\n",
      "$ -> SYM -> symbol\n",
      "0.9 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "an -> DET -> determiner\n",
      "operating -> NOUN -> noun\n",
      "loss -> NOUN -> noun\n",
      "of -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "1.8 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "AWS -> PROPN -> proper noun\n",
      "segment -> NOUN -> noun\n",
      "operating -> NOUN -> noun\n",
      "income -> NOUN -> noun\n",
      "was -> AUX -> auxiliary\n",
      "$ -> SYM -> symbol\n",
      "5.4 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "operating -> VERB -> verb\n",
      "income -> NOUN -> noun\n",
      "of -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "5.7 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "Net -> ADJ -> adjective\n",
      "income -> NOUN -> noun\n",
      "was -> AUX -> auxiliary\n",
      "$ -> SYM -> symbol\n",
      "6.7 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "in -> ADP -> adposition\n",
      "the -> DET -> determiner\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "or -> CCONJ -> coordinating conjunction\n",
      "$ -> SYM -> symbol\n",
      "0.65 -> NUM -> numeral\n",
      "per -> ADP -> adposition\n",
      "diluted -> ADJ -> adjective\n",
      "share -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "with -> ADP -> adposition\n",
      "a -> DET -> determiner\n",
      "net -> ADJ -> adjective\n",
      "loss -> NOUN -> noun\n",
      "of -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "2.0 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      ", -> PUNCT -> punctuation\n",
      "or -> CCONJ -> coordinating conjunction\n",
      "$ -> SYM -> symbol\n",
      "0.20 -> NUM -> numeral\n",
      "per -> ADP -> adposition\n",
      "diluted -> ADJ -> adjective\n",
      "share -> NOUN -> noun\n",
      ", -> PUNCT -> punctuation\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n",
      "\n",
      " -> SPACE -> space\n",
      "Second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2023 -> NUM -> numeral\n",
      "net -> ADJ -> adjective\n",
      "income -> NOUN -> noun\n",
      "includes -> VERB -> verb\n",
      "a -> DET -> determiner\n",
      "pre -> ADJ -> adjective\n",
      "- -> ADJ -> adjective\n",
      "tax -> ADJ -> adjective\n",
      "valuation -> NOUN -> noun\n",
      "gain -> NOUN -> noun\n",
      "of -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "0.2 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "included -> VERB -> verb\n",
      "in -> ADP -> adposition\n",
      "non -> ADJ -> adjective\n",
      "- -> ADJ -> adjective\n",
      "operating -> ADJ -> adjective\n",
      "expense -> NOUN -> noun\n",
      "from -> ADP -> adposition\n",
      "the -> DET -> determiner\n",
      "common -> ADJ -> adjective\n",
      "stock -> NOUN -> noun\n",
      "investment -> NOUN -> noun\n",
      "in -> ADP -> adposition\n",
      "Rivian -> PROPN -> proper noun\n",
      "Automotive -> PROPN -> proper noun\n",
      ", -> PUNCT -> punctuation\n",
      "Inc. -> PROPN -> proper noun\n",
      ", -> PUNCT -> punctuation\n",
      "compared -> VERB -> verb\n",
      "to -> ADP -> adposition\n",
      "a -> DET -> determiner\n",
      "pre -> ADJ -> adjective\n",
      "- -> ADJ -> adjective\n",
      "tax -> ADJ -> adjective\n",
      "valuation -> NOUN -> noun\n",
      "loss -> NOUN -> noun\n",
      "of -> ADP -> adposition\n",
      "$ -> SYM -> symbol\n",
      "3.9 -> NUM -> numeral\n",
      "billion -> NUM -> numeral\n",
      "from -> ADP -> adposition\n",
      "the -> DET -> determiner\n",
      "investment -> NOUN -> noun\n",
      "in -> ADP -> adposition\n",
      "second -> ADJ -> adjective\n",
      "quarter -> NOUN -> noun\n",
      "2022 -> NUM -> numeral\n",
      ". -> PUNCT -> punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,'->',token.pos_,'->',spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74bc87b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 16,\n",
       " 97: 37,\n",
       " 92: 69,\n",
       " 100: 21,\n",
       " 84: 35,\n",
       " 85: 43,\n",
       " 95: 1,\n",
       " 93: 54,\n",
       " 103: 10,\n",
       " 99: 20,\n",
       " 90: 12,\n",
       " 87: 4,\n",
       " 89: 2}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in order to count how many noun, pronoun etc are present in doc..\n",
    "c=doc.count_by(spacy.attrs.POS)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f749ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN -> 16\n",
      "PUNCT -> 37\n",
      "NOUN -> 69\n",
      "VERB -> 21\n",
      "ADJ -> 35\n",
      "ADP -> 43\n",
      "PRON -> 1\n",
      "NUM -> 54\n",
      "SPACE -> 10\n",
      "SYM -> 20\n",
      "DET -> 12\n",
      "AUX -> 4\n",
      "CCONJ -> 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in c.items():\n",
    "    print(doc.vocab[k].text,'->',v)  #counts.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "758b0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp('Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\\n\\nThe consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from Marchâ€™s peak but was still close to the highest level since the summer of 1982.\\n\\nRemoving volatile food and ene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c476b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\n",
      "\n",
      "The consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from Marchâ€™s peak but was still close to the highest level since the summer of 1982.\n",
      "\n",
      "Removing volatile food and ene\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdae07ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Inflation,\n",
       " climb,\n",
       " consumers,\n",
       " brink,\n",
       " expansion,\n",
       " consumer,\n",
       " price,\n",
       " index,\n",
       " measure,\n",
       " prices,\n",
       " goods,\n",
       " services,\n",
       " %,\n",
       " year,\n",
       " estimate,\n",
       " %,\n",
       " gain,\n",
       " ease,\n",
       " Marchâ€,\n",
       " ™,\n",
       " peak,\n",
       " level,\n",
       " summer,\n",
       " food,\n",
       " ene]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun=[]\n",
    "for token in doc:\n",
    "    if token.pos_ in [\"NOUN\"]:\n",
    "        noun.append(token)\n",
    "#         print(token,'->',token.pos_)\n",
    "noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18cdb78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.3, 8.1, 1982]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=[]\n",
    "for token in doc:\n",
    "    if token.pos_ in [\"NUM\"]:\n",
    "        num.append(token)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "613c8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN -> 16\n",
      "PUNCT -> 37\n",
      "NOUN -> 69\n",
      "VERB -> 21\n",
      "ADJ -> 35\n",
      "ADP -> 43\n",
      "PRON -> 1\n",
      "NUM -> 54\n",
      "SPACE -> 10\n",
      "SYM -> 20\n",
      "DET -> 12\n",
      "AUX -> 4\n",
      "CCONJ -> 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in c.items():\n",
    "    print(doc.vocab[k].text,'->',v)  #counts.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63074aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to Train a Spacy NER Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcae48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Elon Musk is the founder of SpaceX.\", {'entities': [(0, 10, 'PERSON'), (29, 34, 'ORG')]}),\n",
    "    (\"The Louvre Museum in Paris displays famous artworks.\", \n",
    "     {'entities': [(4, 17, 'LOCATION'), (31, 46, 'ACTIVITY')]}),\n",
    "    (\"Amazon Web Services (AWS) offers cloud computing solutions.\", \n",
    "     {'entities': [(0, 24, 'ORG'), (38, 55, 'PRODUCT')]}),\n",
    "    (\"Barack Obama served as the 44th President of the United States.\", \n",
    "     {'entities': [(0, 12, 'PERSON'), (40, 58, 'POSITION'), (63, 78, 'LOCATION')]}),\n",
    "    (\"The iPhone 13, produced by Apple, is a popular smartphone.\", \n",
    "     {'entities': [(4, 12, 'PRODUCT'), (28, 33, 'ORG'), (57, 67, 'PRODUCT')]}),\n",
    "    (\"Tokyo Disneyland, located in Urayasu, Japan, is a famous theme park.\", \n",
    "     {'entities': [(0, 15, 'LOCATION'), (30, 37, 'LOCATION'), (49, 60, 'LOCATION'), (68, 78, 'ACTIVITY')]}),\n",
    "    (\"The Nobel Prize is awarded annually for outstanding achievements in various fields.\", \n",
    "     {'entities': [(4, 15, 'EVENT'), (44, 61, 'ACTIVITY')]}),\n",
    "    (\"Harvard University, situated in Cambridge, Massachusetts, is a prestigious institution.\", \n",
    "     {'entities': [(0, 17, 'ORG'), (30, 40, 'LOCATION'), (56, 73, 'DESIGNATION')]}),\n",
    "    (\"The Eiffel Tower in France attracts millions of tourists every year.\", \n",
    "     {'entities': [(4, 15, 'LOCATION'), (19, 25, 'LOCATION'), (54, 64, 'ACTIVITY')]}),\n",
    "    (\"Microsoft Excel is widely used for spreadsheet calculations.\", \n",
    "     {'entities': [(0, 16, 'PRODUCT'), (26, 42, 'ACTIVITY')]}),\n",
    "    (\"Albert Einstein, known for his theory of relativity, was a famous physicist.\", \n",
    "     {'entities': [(0, 13, 'PERSON'), (42, 51, 'ACTIVITY')]}),\n",
    "    (\"The Statue of Liberty, located in New York Harbor, is a symbol of freedom.\", \n",
    "     {'entities': [(4, 21, 'LOCATION'), (34, 50, 'LOCATION'), (59, 66, 'CONCEPT')]}),\n",
    "    (\"Google Maps is a popular navigation app for smartphones.\", \n",
    "     {'entities': [(0, 12, 'PRODUCT'), (42, 52, 'PRODUCT')]}),\n",
    "    (\"The Berlin Wall, once dividing East and West Berlin, fell in 1989.\", \n",
    "     {'entities': [(4, 15, 'LOCATION'), (38, 54, 'EVENT')]}),\n",
    "    (\"Michelle Obama, former First Lady of the United States, promotes education.\", \n",
    "     {'entities': [(0, 14, 'PERSON'), (30, 51, 'POSITION'), (57, 67, 'ACTIVITY')]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7eabc83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ced6445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin  #used for serialization and deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc552419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 908.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity with incorrect format: 29 34 ORG\n",
      "Skipping entity with incorrect format: 63 78 LOCATION\n",
      "Skipping entity with incorrect format: 57 67 PRODUCT\n",
      "Skipping entity with incorrect format: 68 78 ACTIVITY\n",
      "Skipping entity with incorrect format: 44 61 ACTIVITY\n",
      "Skipping entity with incorrect format: 30 40 LOCATION\n",
      "Skipping entity with incorrect format: 19 25 LOCATION\n",
      "Skipping entity with incorrect format: 42 51 ACTIVITY\n",
      "Skipping entity with incorrect format: 42 52 PRODUCT\n",
      "Skipping entity with incorrect format: 57 67 ACTIVITY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "db = DocBin()  # Creating a DocBin object, converting doc file into binary format to easily save the file in disk\n",
    "\n",
    "for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "\n",
    "    for start, end, label in annot.get('entities', []):  # Use .get() to provide a default empty list\n",
    "        try:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "            else:\n",
    "                print(\"Skipping entity with incorrect format:\", start, end, label)\n",
    "        except ValueError:\n",
    "            print(\"Error processing entity:\", start, end, label)\n",
    "\n",
    "    ents=spacy.util.filter_spans(ents)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "os.chdir(r'C:\\Users\\KIIT\\Documents\\Python Scripts\\spacy_v3 TrainingNER')\n",
    "db.to_disk(\"./ner.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b956d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\") # load other spacy model\n",
    "\n",
    "# db = DocBin() # create a DocBin object\n",
    "\n",
    "# for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "#     doc = nlp.make_doc(text) # create doc object from text\n",
    "#     ents = []\n",
    "#     for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "#         span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "#         if span is None:\n",
    "#             print(\"Skipping entity\")\n",
    "#         else:\n",
    "#             ents.append(span)\n",
    "#     doc.ents = ents # label the text with the ents\n",
    "#     db.add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af38fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating config file to training model\n",
    "# can find more on www.spacy.io/usage/training#config, for more components like tagger, parser etc(custom training)\n",
    "\n",
    "\n",
    "# download the config file and store it in the location above used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26624e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now u have to fill the config base file, bcoz many of details reqrd by compenent(ner) might not be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a52a2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m[+] Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_model.cfg config.cfg\n",
    "\n",
    "# you ll find config.cfg file \n",
    "#now that particular config file got filled for training, now all the info will be stored in config.cfg file\n",
    "#additional things got filled like, learn_rate, optimizer etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8818486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4m[i] Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     52.68    0.00    0.00    0.00    0.00\n",
      "100     200         64.46   1856.28  100.00  100.00  100.00    1.00\n",
      "200     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "380     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "580     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "780    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "980    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1180    1400         24.98     12.40  100.00  100.00  100.00    1.00\n",
      "1380    1600         26.90      9.73  100.00  100.00  100.00    1.00\n",
      "1580    1800         31.68      2.70  100.00  100.00  100.00    1.00\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "output\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train ./ner.spacy --paths.dev ./ner.spacy \n",
    "\n",
    "#model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69c5d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"C:/Users/KIIT/Documents/Python Scripts/spacy_v3 TrainingNER/output/model-best\" \n",
    "best_nlp = spacy.load(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4eda8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuvraj Singh -> PERSON -> People, including fictional\n",
      "to America -> LOCATION -> None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Yuvraj Singh\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " went \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    to America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOCATION</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Yuvraj Singh went to America\"\n",
    "best_doc = best_nlp(text)\n",
    "for ent in best_doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_, \"->\", spacy.explain(ent.label_))\n",
    "displacy.render(best_doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faebba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
